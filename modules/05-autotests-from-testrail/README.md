# Инструкция для Cursor: Написание автотестов Playwright на основе тест-кейсов TestRail

## Быстрый старт (копипаст)

```
Открой и строго следуй инструкциям: modules/05-autotests-from-testrail/README.md

Создай автотесты на тесты секции <sectionId>.
Аналитика: <ссылка>
(Опционально) DOM/HTML фрагмент(ы): <...>
(Опционально) Jira задачи на добавление `data-testid`: <DS-...>, <DS-...>
```

## Принцип работы модуля

Ты действуешь **строго по шагам**, которые даёт пользователь.  
Пользователь даёт шаги “что надо делать” — ты добавляешь/уточняешь их формулировки в этом README **без лишних дополнений**.

---

## 1. Алгоритм работы

### Основная команда пользователя

Пользователь задаёт команду в формате:

- `создай автотесты на тесты секции <sectionId>`
- `<ссылка на аналитику>`
- (опционально) `<DOM/HTML фрагмент(ы)>`

`sectionId` может быть указан произвольно — перед созданием Jira-задачи нужно открыть TestRail и понять, какая секция соответствует этому id.

### Согласованный флоу

Ты работаешь в следующем порядке:

1) найти секцию в TestRail по `sectionId` (уточнить название/путь)
2) создать Jira-задачу на автотесты (Project/поля/`[UI DS]`)
3) изучить аналитику
4) изучить тесты (кейсы) в секции TestRail
5) изучить статью Confluence “Recommended E2E Test Format”: `https://starsteer.atlassian.net/wiki/spaces/RTM/pages/5734629408/Recommended+E2E+Test+Format`
6) при необходимости запросить/проверить DOM и `data-testid`
7) если найден `NEEDED data-testid`/`MISSING data-testid` — создать Jira-задачу на добавление атрибутов (см. пример `DS-12116`)
8) написать автотест(ы) по промту (сохранить результат в `results/05-autotests-from-testrail`)
9) обязательно запустить автотест(ы) для проверки

---

### Шаг 1 — Поиск секции в TestRail

Открой TestRail и найди секцию по `sectionId` из команды пользователя (уточни название секции и путь).

---

### Шаг 2 — Создание задачи на написание автотеста

Создай задачу в Jira на написание/обновление автотестов для найденной секции.

Пример задачи:

- `QAA-6200` (`https://starsteer.atlassian.net/browse/QAA-6200`)
  - **Project**: QA Automation
  - **Summary**: `[UI DS] Add Playwright E2E tests for TestRail “Trace list” section`
  - **Assignee**: Aigerim Suleimenova
  - **Labels**: QAA
  - **Dev Team**: DrillSpot
  - **Description**: покрыть автотестами функциональность секции `TestRail → Trace list` (id `1911`), чтобы стабильно проверять ключевые пользовательские сценарии и предотвратить регрессии
  - **Acceptance Criteria**:
    - добавлены **E2E тесты Playwright** для основных сценариев Trace list
    - сценарии сопоставлены с кейсами в TestRail секции “Trace list”

Требование к оформлению шага:

- описывай только то, что нужно сделать (кратко и однозначно)
- не добавляй дополнительных требований/подшагов, которых не было в задаче/в шагах пользователя
- убедись, что в Jira-задаче заполнены поля **Assignee**, **Labels**, **Dev Team**
- убедись, что Jira-задача создана в проекте **QA Automation**
- добавляй префикс **[UI DS]** в **Summary**

---

### Шаг 3 — Изучение аналитики

Открой ссылку на аналитику, которую пользователь приложил к команде, и изучи её.

---

### Шаг 4 — Изучение тестов в секции TestRail

Открой TestRail и изучи тесты в найденной секции.

Дополнительно (обязательно): учти метаданные кейса, которые “подсказывают контекст”.

- Всегда смотри **лейблы/теги** кейса (например: `linear_widget`, `trace_list`, `track` и т.п.).
- Используй их как сигнал к setup’у и ожиданиям в автотесте (что должно быть на дашборде/в окне).
- Если по тексту кейса/expected неочевидно, но лейблы явно намекают на обязательный контекст (например `linear_widget`), **не додумывай молча** — задай пользователю короткий уточняющий вопрос перед генерацией финального кода, например:
  - “В этом сценарии нужно добавить на окно **Linear widget** (лейбл `linear_widget`) или окно должно быть пустым?”

---

### Шаг 5 — Проверка DOM и `data-testid` (при необходимости)

Если DOM/HTML фрагменты не предоставлены и по описанию неясно, есть ли у нужных элементов `data-testid`, сначала выдай:

- `NEEDED DOM:` (какие элементы и какой HTML-фрагмент нужен)
- `NEEDED data-testid:` (какие атрибуты нужны + куда добавить в TSX)

---

### Шаг 6 — Создание Jira-задачи на добавление `data-testid` (если нужно)

Если по результатам проверки выявлен `NEEDED data-testid`/`MISSING data-testid`, создай отдельную Jira-задачу на разметку UI-элементов атрибутами `data-testid`.

Пример задачи:

- `DS-12116` (`https://starsteer.atlassian.net/browse/DS-12116`)
  - **Labels**: autotest
  - **Parent**: `DS-9340 Drillspot. Autotests.`
  - **Summary**: `Drillspot. Autotest. Add test-attribute for UI-elements from Traces List/ Messages`
  - **Description** (шаблон):
    - `Прилагаю список элементов и примерный data-testid:`
      - `<тут список элементов + предлагаемые data-testid>`
    - `Обязательно, чтобы значение атрибута было информативным и без дубликатов (существовало в единственном экзем. в DOM, если это единственный элемент.)`
    - `Screenshots:`
      - `<тут добавить скриншоты>`

---

### Шаг 7 — Написание автотестов

Промт для генерации автотестов:

Ты — QA Automation Engineer в проекте RTM. Сгенерируй Playwright E2E тест(ы) в формате Confluence “Recommended E2E Test Format”.

Перед написанием обязательно изучи статью:

`https://starsteer.atlassian.net/wiki/spaces/RTM/pages/5734629408/Recommended+E2E+Test+Format`

Цель: создать единообразные тесты, читаемые как спецификация, хорошо логирующиеся в HTML report/trace, и легко маппящиеся на TestRail.

Формат (ОБЯЗАТЕЛЬНО):
- Перед `test.describe` добавь ЗАМЕТНЫЙ комментарий со списком задач на `data-testid` (если они известны).
  Предпочтительный формат — многострочный блок:

  ```ts
  /**
   * data-testid tasks (Jira):
   * - DS-12345
   * - DS-67890
   */
  ```

  Если задача одна — всё равно используй этот формат (чтобы комментарий не терялся в коде).
- `test.describe('<UI area> > <context>', () => { ... })`
  - строка describe = “область UI + контекст сценария”, берём из TestRail (если есть).
- Имя теста: `ABC-### <TestRail case title> [<caseId>]`
  ABC — внутренний префикс фичи/модуля (2–5 символов), который используется для идентификации набора тестов и их нумерации внутри фичи.
Примеры: TL = Trace List, AUTH = Authentication, DASH = Dashboards, LP = Left Panel.
ABC-### — внутренняя нумерация тестов в рамках выбранной фичи, не связана с TestRail ID.
Пример: TL-001 ... [61960], TL-002 ... [61966].
  - `<TestRail case title>` — точный title из TestRail.
  - `[<caseId>]` — числовой id кейса из TestRail в квадратных скобках.
- Внутри теста ровно ОДИН `await test.step('<expected result>', async () => { ... })`
  - `<expected result>` = Expected Result из TestRail, сжать до одной короткой фразы (без деталей реализации).
- Внутри `test.step`:
  - только нумерованные проверки `// Assert 1: ...`, `// Assert 2: ...`, ...
  - формулировки Assert — про ОЖИДАНИЕ, не про реализацию.
- Ассерты в автотесте должны соответствовать проверкам в тесте TestRail (expected/проверки из кейса должны быть отражены в asserts; если что-то не автоматизируется — это явно фиксируется)
- TypeScript:
  - НЕ использовать `any`
  - типизировать фикстуры/моки, где возможно использовать `satisfies` для массивов/объектов.
- Стабильность:
  - тест не должен зависеть от порядка данных/случайных таймингов
  - использовать `expect(...).toBeVisible()`/`waitForLoadState('domcontentloaded')`/`waitForURL` где оправдано.

ОЧЕНЬ ВАЖНО (STRICT): локаторы и DOM. Следовать без исключений.
Ты ОБЯЗАН соблюдать внутреннее правило из DS-9203: для автотестов локаторы должны быть на `data-testid`.

Правила:
- DOM-first: перед тем как писать тест, убедись, что у всех нужных элементов есть `data-testid`.
- Если DOM/HTML фрагмент не предоставлен и по описанию неясно, есть ли `data-testid`, сначала выдай список `NEEDED DOM:` (какие элементы и какой кусок HTML нужен), и только потом генерируй финальные локаторы/тест.
- Локаторы (строго):
  - по умолчанию использовать ТОЛЬКО `getByTestId`: `page.getByTestId('...')`.
  - `getByRole` разрешён только как временный fallback и только если:
    - он уникален и стабилен,
    - и используется внутри контейнера, найденного по `data-testid` (пример: `page.getByTestId('panel').getByRole('button', { name: '...' })`).
  - запрещено строить локаторы на:
    - CSS-классах (в т.ч. ui-kit/css-modules),
    - тексте (если текст может меняться/локализован),
    - порядке DOM, `.nth()`/`.first()`/`.last()` как “основе” локатора,
    - “магических” селекторах по layout/глубине вложенности.
  - CSS/XPath — только в крайнем случае (и обязательно объясни, почему иначе нельзя).
- Если нужного `data-testid` нет в приложении — это MUST:
  - явно отметь: `MISSING data-testid: <element>`
  - предложи конкретное имя в согласованном формате: `feature.element.action` (информативно, без дублей).
  - объясни куда именно добавить атрибут в компоненте (на какой элемент: кнопка/контейнер/строка списка).
  - для повторяющихся элементов: добавь суффикс `-${id}` (`feature.list.item-${uuid}`), чтобы обеспечить уникальность.
- UI-kit / portal элементы:
  - сначала цепляйся за ваш триггер/контейнер с `data-testid`,
  - затем локализуйся внутри портала максимально стабильно,
  - в ui-kit “лезем” только если вообще невозможно стабильно протестировать иначе (тогда это отдельная задача).

В конце ответа (если не хватает атрибутов) всегда добавляй секции:
- `NEEDED data-testid:` (имя + место в TSX)
- `NEEDED DOM:` (какие элементы и какой HTML-фрагмент нужен, чтобы написать точные локаторы)

Импорт/организация:
- Используй `import { test, expect } from '../../fixtures'` (или корректный relative path по месту файла).
- Вспомогательные функции (setup, локаторы, assert-helpers) выноси в верх файла и документируй JSDoc (HOW/WHY).
- Не меняй существующие тесты; добавляй новый тест + новые helper’ы только при необходимости.

Входные данные (я предоставляю):
- Feature area: {например "Authentication", "Dashboards", "Settings", ...}
- UI area/context для describe: {например "Login > Basic flow"}
- File path где разместить тест: {например "playwright/tests/Auth/login.spec.ts"}
- TestRail case title: {точный title}
- TestRail case id: {число}
- Case steps + expected result: {текст из TestRail}
- Preconditions: {роль, фича-флаг, тестовые данные, моки/стабилизация}
- Actions: {какие клики/ввод/навигация}
- Assertions: {список ожиданий в правильном порядке}
- Mocking strategy: {none / api route mocks / fixtures / websocket mock (если применимо)}

Сгенерируй:
- Готовый код теста(ов) в нужный файл (если файла нет — создай).
- 1 тест = 1 `test.step`.
- Внутри шага: Arrange/Act делай как код, но ассерты оформляй как `// Assert N:` + `expect`.
- Только код/дифф без лишних объяснений.

Файл(ы) создавать в папке:
`C:\Users\a.suleimenova_rogii\Documents\my AI Assistant\my-assistant\results\05-autotests-from-testrail`

---

### Запуск автотеста(ов) для проверки (обязательно)

Важно: запускать **из папки** `C:\work\solo-rtm\playwright`.

**Запуск одного кейса по TestRail ID**

```bash
cmd /c "set HEADED_SLOW=1&& npx playwright test tests/LeftPanel/trace-list.spec.ts -g 81108 --reporter=line --headed --workers=1"
```

**Запуск по имени TL (если нужно)**

```bash
cmd /c "set HEADED_SLOW=1&& npx playwright test tests/LeftPanel/trace-list.spec.ts -g \"TL-014\" --reporter=line --headed --workers=1"
```

Если тест упал:

- не “подгоняй” автотест под текущее поведение молча
- в ответе пользователю перечисли, **какие Assert N не прошли**, чтобы можно было решить: это баг в продукте или неактуальный TestRail тест



