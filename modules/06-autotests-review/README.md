# Инструкция для Cursor: Ревью Playwright автотестов

## Быстрый старт (копипаст)

```
Открой и строго следуй инструкциям: modules/06-autotests-review/README.md

Проведи ревью автотестов Playwright.
Файлы/папки для ревью: tests from solo-rtm/LeftPanel/trace-list.spec.ts
Jira задачи на добавление `data-testid`: <DS-...>, <DS-...>
(Опционально) TestRail секция/кейс(ы): <sectionId или список caseId>
```

Важно:
- Не предлагай лишнего. Пиши только реальные проблемы и конкретные улучшения.
- Не меняй код, если пользователь явно не попросил.

---

## 0. Роль и принципы

Ты — опытный **QA Automation Engineer**. Цель ревью — повысить **стабильность**, **читабельность** и **поддерживаемость** автотестов, а также соответствие тестам/требованиям (если они приложены).

---

## 1. Критерии ревью (что проверять)

### Важно: `skip` тесты

- Тесты, помеченные как `skip`, нужно **пропускать** во время ревью (не включать в замечания/рекомендации/подсчёты).
- Считай “skip” любым из вариантов: `test.skip(...)`, `test.describe.skip(...)`

### 0) Соответствие “эталону” проекта (лиды)

- Сверяй стиль/структуру/паттерны с эталонными тестами (см. раздел 1.1).
- Если есть противоречие между “как написано в проекте” и “как хотелось бы” — ориентируйся на **эталон лидов** и отмечай расхождения как рекомендации, если это не ломает качество/стабильность.

### 1) Соответствие требованиям / TestRail (если есть)

- Если в названии теста есть `[<caseId>]` — **обязательно** подтяни кейс из TestRail через MCP (`get_case`) и используй его как source of truth.
- Ассерты должны соответствовать проверкам/expected из TestRail или требований.
- Проверь **полное соответствие** автотеста тест-кейсу TestRail:
  - title / смысл сценария
  - предусловия (что требуется для выполнения кейса) ↔ Arrange/данные в тесте
  - шаги/expected ↔ действия и проверки в тесте
  - негативные/граничные проверки — если они есть в TestRail, они должны быть отражены в автотесте (или явно помечены как “не автоматизируемо”)
- Если найдено расхождение: **не “подгоняй” тест молча**, а фиксируй как риск: *“возможно баг / возможно устаревший TR-кейс”*.

Дополнительно (обязательно) проверь метаданные кейса в TestRail:

Используй ID значений из `testrail-field-ids.md`.

- **Status**: Ready (`case_status_id = 1`)
- **Automation**: Automated (`custom_automation = 2`)
- **Automation Type**: TypeScript (`custom_automation_type = 5`)
- **Type**: Regression (`type_id = 9`)
- **Assigned To**: None (`case_assignedto_id` должен быть пустым/`null`)

И проверь, что в кейсе в TestRail добавлен корректный комментарий/пометка автотеста в формате:

`Autotest: tests/LeftPanel/trace-list.spec.ts::TL-001 Dashboard without solo-well/wellbore [61960]`

### 2) Локаторы и селекторы

- Использовать стабильные локаторы (`data-testid`) по правилам проекта.
- Запрещено строить локаторы на нестабильных признаках (случайный текст, CSS-классы, `.nth()` как основа и т.п.), если есть альтернатива.
- Если пользователь дал Jira-задачи на добавление `data-testid`:
  - собери все места, где тест не может использовать `data-testid` (локаторы по тексту/CSS/XPath/`nth` и т.п.)
  - сверь, что каждый такой элемент/контрол описан в этих Jira-задачах
  - если нашёл элементы без `data-testid`, которые **не покрыты** задачами — верни их списком в отчёте (чтобы завести/дополнить задачи)

### 3) Надёжность и флейки

- Нет зависимостей от таймингов/случайного порядка данных.
- Нет лишних ожиданий, но есть необходимые `waitFor…`/`expect…` для стабильности.
- Повторяемость: тест должен быть детерминированным.

### 4) Структура теста и читаемость

- Понятный `describe`/`test` нейминг.
- `test.step` используется осознанно (если проект требует 1 step — проверяем соблюдение).
- Ассерты оформлены как явные проверки (в том числе `Assert N` если это правило).

### 5) Поддерживаемость

- Повторяющийся код вынесен в helper’ы/fixtures, но без “over-engineering”.
- Нет мёртвого кода, закомментированных блоков, лишних логов.
- Типизация: без `any`.

---

## 1.1 Эталонные автотесты (лиды) — обязательно изучить и использовать как baseline

Эти тесты написаны лидами и считаются **эталонными**:

- `tests from solo-rtm/trajectory`
- `tests from solo-rtm/Routing`

Что нужно извлечь из “эталона” и применять в ревью как стандарт:

- **Нейминг**: формат `XX-001: ...` / наличие ссылок на DS/ID в названии.
- **Структура**: когда используют `test.describe.serial`, когда `beforeAll`/`afterAll` и когда `beforeEach`/`afterEach`.
- **Page Objects / helpers**: как выносят логику (страницы/обёртки) и как организуют тест-данные/константы.
- **Моки/стабы**: использование mock-системы и предотвращение “pollution between tests”.
- **Локаторы**: приоритет `getByTestId` и какие допускаются исключения (например, стабильные `data-*` селекторы).
- **Ожидания**: какие `expect` и `timeout` используют для стабильности.

Отдельным пунктом в результате ревью всегда выдай:

- **“Рекомендации по стилю/паттернам по итогам изучения эталона”** — 5–15 буллетов, применимых к текущим файлам.

---

## 2. Формат результата (как отвечать)

### 2.0 Проверка соответствия TestRail (если есть caseId/секция)

Добавь отдельный блок (по каждому `caseId`), но показывай **только то, что NOT OK** (OK пункты не выводи).

- **Case <caseId>**:
  - **TR ↔ Autotest mapping**: `<specPath>::<testName>`
  - **NOT OK**:
    - Full match: <коротко что не совпадает>
    - Status (`case_status_id = 1`): <фактическое значение/ID>
    - Automation (`custom_automation = 2`): <фактическое значение/ID>
    - Automation Type (`custom_automation_type = 5`): <фактическое значение/ID>
    - Type (`type_id = 9`): <фактическое значение/ID>
    - Assigned To (`case_assignedto_id = null`): <фактическое значение/ID>
    - Autotest comment in TR: <что сейчас и какой должен быть>

### 2.0 Проверка покрытия задач по `data-testid` (если задачи даны)

Добавь отдельный блок:

- **Covered by Jira tasks**: список элементов/контролов без `data-testid`, которые уже перечислены в задачах
- **Missing in Jira tasks**: список элементов/контролов без `data-testid`, которые не перечислены (их нужно добавить в задачи)

### 2.1. По каждому файлу

Для каждого файла:

- **Кратко**: что хорошо
- **Проблемы**: список замечаний (каждое с уровнем важности)
- **Рекомендации**: что конкретно поправить (с примерами/фрагментами)

Уровни важности:

- **Critical** — тест может давать ложные результаты / флейки / вообще не проверяет нужное
- **Major** — снижает стабильность/поддерживаемость
- **Minor** — стиль/читаемость

### 2.2. Финальная сводка

- Кол-во файлов: X
- Skipped tests: N (skip-тесты не ревьюим, но можно учитывать в метриках)
- Critical: N
- Major: N
- Minor: N
- Топ-3 самых важных правки

### 2.3. Сохранение отчёта

Сохрани отчёт в папку:

`results/06-autotests-review/`

Имя файла:

Не перезаписывай прошлые ревью. Каждая итерация ревью должна сохраняться отдельным файлом.

Формат имени:

`dd_mm_yy_<specFile>_autotests_review_<hhmm>.md`

Где:

- `<specFile>` — имя spec-файла (например, `trace-list.spec.ts`)
- `<hhmm>` — время (24h), чтобы несколько ревью в один день не перетирали друг друга

Если ревью по нескольким файлам — используй `multi` вместо `<specFile>`.

### 2.4. Следующий шаг

В конце отчёта добавь:

- Внеси правки согласно рекомендациям из отчёта.
- После внесения правок прогони автотест(ы) и проверь, что они проходят успешно.

---

## 3. Правило про исправления

- Если пользователь не просил — **не вноси изменения в код**, только дай рекомендации.
- Если пользователь просит исправить — сначала предложи план правок и после этого правь.

